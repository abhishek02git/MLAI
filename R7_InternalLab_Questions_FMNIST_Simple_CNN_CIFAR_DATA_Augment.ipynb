{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "R7_InternalLab_Questions_FMNIST_Simple_CNN_CIFAR_DATA_Augment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GreatLearningAIML1/gurgaon-feb-batch-abhishek02git/blob/master/R7_InternalLab_Questions_FMNIST_Simple_CNN_CIFAR_DATA_Augment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyfMmMnPJjvn",
        "colab_type": "text"
      },
      "source": [
        "## Train a simple convnet on the Fashion MNIST dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjcGOJhcJjvp",
        "colab_type": "text"
      },
      "source": [
        "In this, we will see how to deal with image data and train a convnet for image classification task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSl7_5tW7JRW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy\n",
        "import time\n",
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jR0Pl2XjJjvq",
        "colab_type": "text"
      },
      "source": [
        "### Load the  `fashion_mnist`  dataset\n",
        "\n",
        "** Use keras.datasets to load the dataset **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qr75v_UYJjvs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "2979122e-00f5-491b-88ec-f228abd845c6"
      },
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 3us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 2s 0us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTI42-0qJjvw",
        "colab_type": "text"
      },
      "source": [
        "### Find no.of samples are there in training and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2sf67VoJjvx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c03dd9a2-1c04-4a3c-a2f8-386ed617cd6a"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zewyDcBlJjv1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cc71c642-a627-4500-bd86-ce98ac8ea055"
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WytT2eRnJjv4",
        "colab_type": "text"
      },
      "source": [
        "### Find dimensions of an image in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XycQGBSGJjv5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eb6da05e-ae74-4896-82ff-a84bdeb18f94"
      },
      "source": [
        "X_train[0].shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jtdZ7RqJjv8",
        "colab_type": "text"
      },
      "source": [
        "### Convert train and test labels to one hot vectors\n",
        "\n",
        "** check `keras.utils.to_categorical()` **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAD3q5I6Jjv9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train_cat = keras.utils.to_categorical(y_train)\n",
        "y_test_cat = keras.utils.to_categorical(y_test)\n",
        "#num_classes = y_test.shape[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO5BRBzBJjwD",
        "colab_type": "text"
      },
      "source": [
        "### Normalize both the train and test image data from 0-255 to 0-1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fUQpMHxJjwE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# normalize inputs from 0-255 to 0.0-1.0\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da5-DwgrJjwM",
        "colab_type": "text"
      },
      "source": [
        "### Reshape the data from 28x28 to 28x28x1 to match input dimensions in Conv2D layer in keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPGVQ-JJJjwN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Will reshape the data in Model\n",
        "X_train = X_train.reshape(60000,28,28,1)\n",
        "X_test = X_test.reshape(10000,28,28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFRRTJq8JjwQ",
        "colab_type": "text"
      },
      "source": [
        "### Import the necessary layers from keras to build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWTZYnKSJjwR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.constraints import maxnorm\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "from keras import backend as K\n",
        "from keras.callbacks import Callback"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C18AoS7eJjwU",
        "colab_type": "text"
      },
      "source": [
        "### Build a model \n",
        "\n",
        "** with 2 Conv layers having `32 3*3 filters` in both convolutions with `relu activations` and `flatten` before passing the feature map into 2 fully connected layers (or Dense Layers) having 128 and 10 neurons with `relu` and `softmax` activations respectively. Now, using `categorical_crossentropy` loss with `adam` optimizer train the model with early stopping `patience=5` and no.of `epochs=10`. **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DORCLgSwJjwV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "0da4bde9-729c-40a8-8452-47b1649dae28"
      },
      "source": [
        "# Create the model\n",
        "model1 = Sequential()\n",
        "model1.add(Conv2D(32,(3,3),input_shape=(28,28,1),padding='same',activation='relu'))\n",
        "model1.add(Conv2D(33,(3,3),padding='same',activation='relu'))\n",
        "\n",
        "model1.add(Flatten())\n",
        "model1.add(Dense(128,activation='relu'))\n",
        "model1.add(Dense(10,activation='softmax'))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0728 10:06:44.148784 140369916417920 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0728 10:06:44.189766 140369916417920 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0728 10:06:44.198187 140369916417920 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TV13iADdAUVY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "d13438ef-dc71-4ad8-c15a-3577368cca90"
      },
      "source": [
        "model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model1.summary())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0728 10:06:46.005420 140369916417920 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0728 10:06:46.032574 140369916417920 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 28, 28, 32)        320       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 28, 28, 33)        9537      \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 25872)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               3311744   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 3,322,891\n",
            "Trainable params: 3,322,891\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyO6T6qPAw2q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "es = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=10)\n",
        "epochs = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwdo8KS9BYmy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "a1fdafc9-0ae8-4c16-fc83-de2e3304b023"
      },
      "source": [
        "start = time.clock() \n",
        "model1.fit(X_train, y_train_cat, validation_data=(X_test, y_test_cat), epochs=epochs, batch_size=32,callbacks=[es])\n",
        "end = time.clock()\n",
        "print(\"Train Time: {} \".format(end-start)) "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0728 10:06:51.089404 140369916417920 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0728 10:06:51.148888 140369916417920 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 20s 330us/step - loss: 0.3559 - acc: 0.8720 - val_loss: 0.2883 - val_acc: 0.8897\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 13s 213us/step - loss: 0.2181 - acc: 0.9204 - val_loss: 0.2504 - val_acc: 0.9069\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 13s 212us/step - loss: 0.1534 - acc: 0.9435 - val_loss: 0.2457 - val_acc: 0.9169\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 13s 214us/step - loss: 0.1044 - acc: 0.9613 - val_loss: 0.2759 - val_acc: 0.9156\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 13s 213us/step - loss: 0.0686 - acc: 0.9745 - val_loss: 0.3082 - val_acc: 0.9171\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 13s 214us/step - loss: 0.0460 - acc: 0.9839 - val_loss: 0.3529 - val_acc: 0.9108\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 13s 214us/step - loss: 0.0308 - acc: 0.9893 - val_loss: 0.4318 - val_acc: 0.9127\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 13s 213us/step - loss: 0.0242 - acc: 0.9913 - val_loss: 0.4388 - val_acc: 0.9165\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 13s 216us/step - loss: 0.0197 - acc: 0.9933 - val_loss: 0.4756 - val_acc: 0.9118\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 13s 216us/step - loss: 0.0174 - acc: 0.9941 - val_loss: 0.4642 - val_acc: 0.9129\n",
            "Train Time: 159.684315 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ju69vKdIJjwX",
        "colab_type": "text"
      },
      "source": [
        "### Now, to the above model add `max` pooling layer of `filter size 2x2` and `dropout` layer with `p=0.25` after the 2 conv layers and run the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2hAP94vJjwY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "4f5d9227-4786-4ba4-d439-9329ebea4648"
      },
      "source": [
        "# Create the model\n",
        "model2 = Sequential()\n",
        "model2.add(Conv2D(32,(3,3),input_shape=(28,28,1),padding='same',activation='relu'))\n",
        "model2.add(Conv2D(33,(3,3),padding='same',activation='relu'))\n",
        "model2.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model2.add(Dropout(rate=0.25))\n",
        "\n",
        "model2.add(Flatten())\n",
        "model2.add(Dense(128,activation='relu'))\n",
        "model2.add(Dropout(rate=0.25))\n",
        "model2.add(Dense(10,activation='softmax'))\n",
        "\n",
        "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model1.summary())\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 28, 28, 32)        320       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 28, 28, 33)        9537      \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 25872)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               3311744   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 3,322,891\n",
            "Trainable params: 3,322,891\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGTA3bfEJjwa",
        "colab_type": "text"
      },
      "source": [
        "### Now, to the above model, lets add Data Augmentation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZ7OYtNqD-vZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "0007bbfc-2fb8-4622-9dbc-17a86b39084e"
      },
      "source": [
        "start = time.clock() \n",
        "model2.fit(X_train, y_train_cat, validation_data=(X_test, y_test_cat), epochs=epochs, batch_size=32,callbacks=[es])\n",
        "end = time.clock()\n",
        "print(\"Train Time: {} \".format(end-start)) "
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 11s 185us/step - loss: 0.4090 - acc: 0.8534 - val_loss: 0.2900 - val_acc: 0.8936\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 10s 174us/step - loss: 0.2684 - acc: 0.9026 - val_loss: 0.2425 - val_acc: 0.9104\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 11s 176us/step - loss: 0.2235 - acc: 0.9162 - val_loss: 0.2382 - val_acc: 0.9140\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 10s 174us/step - loss: 0.1926 - acc: 0.9274 - val_loss: 0.2307 - val_acc: 0.9180\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 11s 179us/step - loss: 0.1679 - acc: 0.9374 - val_loss: 0.2130 - val_acc: 0.9234\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 10s 174us/step - loss: 0.1498 - acc: 0.9435 - val_loss: 0.2390 - val_acc: 0.9169\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 10s 174us/step - loss: 0.1323 - acc: 0.9503 - val_loss: 0.2559 - val_acc: 0.9149\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 11s 177us/step - loss: 0.1218 - acc: 0.9542 - val_loss: 0.2155 - val_acc: 0.9314\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 10s 172us/step - loss: 0.1072 - acc: 0.9586 - val_loss: 0.2325 - val_acc: 0.9301\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 10s 172us/step - loss: 0.0982 - acc: 0.9627 - val_loss: 0.2357 - val_acc: 0.9269\n",
            "Train Time: 136.04431499999998 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6gX8n5SJjwb",
        "colab_type": "text"
      },
      "source": [
        "### Import the ImageDataGenrator from keras and fit the training images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cbz4uHBuJjwc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3bOCuk6F24K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datagen = ImageDataGenerator(featurewise_center=True,\n",
        "    featurewise_std_normalization=True,\n",
        "    rotation_range=90,\n",
        "    width_shift_range=0,\n",
        "    height_shift_range=0,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "datagen.fit(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pl-8dOo7Jjwf",
        "colab_type": "text"
      },
      "source": [
        "#### Showing 5 versions of the first image in training dataset using image datagenerator.flow()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "DpI1_McYJjwg",
        "colab_type": "code",
        "outputId": "43023716-e595-45a7-831c-ff57b19fcac2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "gen = datagen.flow(X_train[0:1], batch_size=1)\n",
        "for i in range(1, 6):\n",
        "    plt.subplot(1,5,i)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(gen.next().squeeze(), cmap='gray')\n",
        "    plt.plot()\n",
        "plt.show()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAABcCAYAAABz9T77AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nB8oX1cn8TuPAAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmPl5yE8Jjwm",
        "colab_type": "text"
      },
      "source": [
        "### Run the above model using fit_generator()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44ZnDdJYJjwn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "outputId": "d737a520-7a23-475d-e31c-161b6eb7cf39"
      },
      "source": [
        "model2.fit_generator(datagen.flow(X_train,y_train_cat,batch_size=10),nb_epoch=10, samples_per_epoch = X_train.shape[0],validation_data=(X_test, y_test_cat))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "  17/6000 [..............................] - ETA: 51s - loss: 11.5573 - acc: 0.1941"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., validation_data=(array([[[..., steps_per_epoch=6000, epochs=10)`\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "6000/6000 [==============================] - 44s 7ms/step - loss: 1.1698 - acc: 0.6140 - val_loss: 2.6342 - val_acc: 0.3221\n",
            "Epoch 2/10\n",
            "6000/6000 [==============================] - 44s 7ms/step - loss: 0.7377 - acc: 0.7343 - val_loss: 2.4628 - val_acc: 0.3306\n",
            "Epoch 3/10\n",
            "6000/6000 [==============================] - 44s 7ms/step - loss: 0.6359 - acc: 0.7730 - val_loss: 2.2678 - val_acc: 0.3254\n",
            "Epoch 4/10\n",
            "6000/6000 [==============================] - 43s 7ms/step - loss: 0.5783 - acc: 0.7923 - val_loss: 2.4363 - val_acc: 0.3652\n",
            "Epoch 5/10\n",
            "6000/6000 [==============================] - 43s 7ms/step - loss: 0.5403 - acc: 0.8052 - val_loss: 2.7644 - val_acc: 0.3007\n",
            "Epoch 6/10\n",
            "6000/6000 [==============================] - 44s 7ms/step - loss: 0.5207 - acc: 0.8139 - val_loss: 2.4146 - val_acc: 0.2917\n",
            "Epoch 7/10\n",
            "6000/6000 [==============================] - 44s 7ms/step - loss: 0.4950 - acc: 0.8231 - val_loss: 2.5523 - val_acc: 0.3198\n",
            "Epoch 8/10\n",
            "6000/6000 [==============================] - 43s 7ms/step - loss: 0.4858 - acc: 0.8271 - val_loss: 2.0523 - val_acc: 0.3437\n",
            "Epoch 9/10\n",
            "6000/6000 [==============================] - 43s 7ms/step - loss: 0.4761 - acc: 0.8293 - val_loss: 2.2065 - val_acc: 0.2871\n",
            "Epoch 10/10\n",
            "6000/6000 [==============================] - 43s 7ms/step - loss: 0.4697 - acc: 0.8330 - val_loss: 2.2683 - val_acc: 0.2991\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa9947fcef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwQQW5iOJjwq",
        "colab_type": "text"
      },
      "source": [
        "###  Report the final train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBwVWNQC2qZD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KXqmUDW2rM1",
        "colab_type": "text"
      },
      "source": [
        "## **DATA AUGMENTATION ON CIFAR10 DATASET**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mja6OgQ3L18",
        "colab_type": "text"
      },
      "source": [
        "One of the best ways to improve the performance of a Deep Learning model is to add more data to the training set. Aside from gathering more instances from the wild that are representative of the distinction task, we want to develop a set of methods that enhance the data we already have. There are many ways to augment existing datasets and produce more robust models. In the image domain, these are done to utilize the full power of the convolutional neural network, which is able to capture translational invariance. This translational invariance is what makes image recognition such a difficult task in the first place. You want the dataset to be representative of the many different positions, angles, lightings, and miscellaneous distortions that are of interest to the vision task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HzVTPUM3WZJ",
        "colab_type": "text"
      },
      "source": [
        "### **Import neessary libraries for data augmentation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPM558TX4KMb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6hicLwP4SqY",
        "colab_type": "text"
      },
      "source": [
        "### **Load CIFAR10 dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQ1WzrXd4WNk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9Pht1ggHuiT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3n28ccU6Hp6s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JN3vYYhK4W0u",
        "colab_type": "text"
      },
      "source": [
        "### **Create a data_gen funtion to genererator with image rotation,shifting image horizontally and vertically with random flip horizontally.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJbekTKi4cmM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-SLtUhC4dK2",
        "colab_type": "text"
      },
      "source": [
        "### **Prepare/fit the generator.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSw8Bv2_4hb0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYyF-P8O4jQ8",
        "colab_type": "text"
      },
      "source": [
        "### **Generate 5 images for 1 of the image of CIFAR10 train dataset.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXug4z234mwQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}